{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing some tools things to begin\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's quickly load our data\n",
    "train_path = 'train.csv'\n",
    "\n",
    "def load_data(path, drop_cols=None):\n",
    "    if drop_cols:\n",
    "        df = pd.read_csv(path, header=0)\n",
    "        df = df.drop(drop_cols, axis=1)\n",
    "        return df\n",
    "    return pd.read_csv(path, header=0)\n",
    "\n",
    "train = load_data(train_path, drop_cols=['ID']) #We don't need the IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to get some basic information from our dataset like\n",
    "what type of information we can find and what our target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float64      1\n",
       "int64      368\n",
       "object       8\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What does our training data look like?\n",
    "train.get_dtype_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have mostly numerical values in our dataset. We also\n",
    "have 8 categorical columns. Since we are using the sci-kit learn library to train a model, we'll need to convert these values to numerical values. We'll want to one hot encode these columns --> make every category into a binary 1 or 0 value. This prevents the magnitude of the numerical variables from affecting your model. We only want the presence of a category as the feature. Keep this in mind for later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    4209.000000\n",
       "mean      100.669318\n",
       "std        12.679381\n",
       "min        72.110000\n",
       "25%        90.820000\n",
       "50%        99.150000\n",
       "75%       109.010000\n",
       "max       265.320000\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What does our target vector look like?\n",
    "train['y'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, the value of our target vector is ~100. No values fall below 72.11 and most of the data lies between ~90 and ~109. We can see that this bell curve also has a heavy tail because the maximum value we can get is 265.32 most of the data lies around ~100. Let's visualize it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  71.,   69.,   27.,  178.,  831.,  543.,  387.,  372.,  361.,\n",
       "         545.,  359.,  191.,  104.,   49.,   41.,   20.,   20.,   14.,\n",
       "           5.,    5.,    5.,    5.,    3.,    0.,    2.,    1.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    1.]),\n",
       " array([  72.11  ,   75.9742,   79.8384,   83.7026,   87.5668,   91.431 ,\n",
       "          95.2952,   99.1594,  103.0236,  106.8878,  110.752 ,  114.6162,\n",
       "         118.4804,  122.3446,  126.2088,  130.073 ,  133.9372,  137.8014,\n",
       "         141.6656,  145.5298,  149.394 ,  153.2582,  157.1224,  160.9866,\n",
       "         164.8508,  168.715 ,  172.5792,  176.4434,  180.3076,  184.1718,\n",
       "         188.036 ,  191.9002,  195.7644,  199.6286,  203.4928,  207.357 ,\n",
       "         211.2212,  215.0854,  218.9496,  222.8138,  226.678 ,  230.5422,\n",
       "         234.4064,  238.2706,  242.1348,  245.999 ,  249.8632,  253.7274,\n",
       "         257.5916,  261.4558,  265.32  ]),\n",
       " <a list of 50 Patch objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqoeD4wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd483b3a510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train['y'], bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing what our target vector looks like helps us keep into context what our predictions should look like. If we get values that look remotely like this, we know we're getting somewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start diving in and trying some basic techniques.\n",
    "We'll start by dividing our data into a training set and a validation set. A validation set allows us to score our model after training it using the training set. The validation set is a small subset of the training data (I normally choose 20% of the training data for a validation set). It is put away not to be seen by the model so we can be sure the model is being tested on new data for which we have labels to score. \n",
    "We're going to be using the ml_tools library which you can find here:\n",
    "https://github.com/mayorquinmachines/ml_tools\n",
    "\n",
    "We'll load the data again using the DataPrep class so that we can load all of our sets and preprocess the data with some basic transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirk/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from ml_tools.dataprep import DataPrep\n",
    "#\"y\" is our target vector, and \"ID\" are ids we don't need for training\n",
    "prep = DataPrep(dummy_pipe=True, target=\"y\", identifier=\"ID\") \n",
    "X_train, y_train, X_val, y_val, test, test_id = prep.load_data('train.csv', 'test.csv')\n",
    "X_train, X_val, test = prep.transform(X_train, X_val, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Awesome! We have all of our data sets transformed uniformly and split into the categories needed. If you look into the dataprep.py script of the ml_tools package, you'll see that it takes several steps to load, split, and transform the data. Let's go through the steps it took:\n",
    "\n",
    "* We initialized the class with the dummy_pipe flag set to True. We saw earlier that our data had categorical data and we were going to need to one hot encode those. \n",
    "\n",
    "* The load_data() method loads both train and test file paths and uses sci-kit learn's train_test_split. This method randomly shuffles your data and splits it into what we call the X_train and X_val sets with their respective y vectors.\n",
    "\n",
    "* When we call the transform() on the training, validation, and test sets, DataPrep remembers that we wanted to use the dummy_pipe. The dummy_pipe is part of the Pipes() class. This pipe will:\n",
    "    * Find all the common columns for all the data sets\n",
    "    * Apply Dummifier(), a custom transformer that will one hot encode all of the categorical columns\n",
    "    * Apply DataSelector(), a custom transformer that will transform a pandas dataframe to a numpy array. \n",
    "    * Apply StandardScaler(), a sci-kit learn transformer that will standardize our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a very basic foundation, we can start testing some models. Here are some good starters to play with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "forest = RandomForestRegressor(n_jobs=-1)\n",
    "gradient_boost = GradientBoostingRegressor()\n",
    "elastic_net = ElasticNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5,\n",
       "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train some models\n",
    "forest.fit(X_train, y_train)\n",
    "gradient_boost.fit(X_train, y_train)\n",
    "elastic_net.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get some predictions using the validation set\n",
    "forest_preds = forest.predict(X_val)\n",
    "gradient_preds = gradient_boost.predict(X_val)\n",
    "elastic_preds = elastic_net.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#How well did they do?\n",
    "from sklearn.metrics import r2_score\n",
    "forest_score = r2_score(y_val, forest_preds)\n",
    "gradient_score = r2_score(y_val, gradient_preds)\n",
    "elastic_score = r2_score(y_val, elastic_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.385519266295 0.570160502223 0.567177813899\n"
     ]
    }
   ],
   "source": [
    "print forest_score, gradient_score, elastic_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like some of these models didn't do half bad with default parameters! From here there's plenty you can do (like going back and preprocessing your data in a new way!). For now, let's experiment and see what we can do by tuning hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use sci-kit learn's GridSearchCV! Grid search allows you to search through a set of parameters. For example:\n",
    "\n",
    "    Say we want to find some good hyperparameters for the random forest model. For this example, we have two parameters we'd like to play with: n_estimators and n_components.\n",
    "    \n",
    "    * for n_estimators we want to test these values: [100,200,300]\n",
    "    * for n_components we want to test these values: [50,100,150]\n",
    "    \n",
    "    If we choose to do a 5-fold cross validation, just for this model we have length(n_estimatos) x length(n_components) = 9 combinations of hyperparameters * 5 rounds of training = 45 models!\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, GridSearchCV makes it easy to test many combinations of hyperparameters very thoroughly. However these calculations take a lot of time. Hyperparameter sweeps are best to be done after doing some more data wrangling and selecting the best combination of features, or potentially getting more data. Tuning parameters should give you the cherry on top. Here's a quick demostration on how long these calculations can take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest search time: 98.0270929337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Let's test how long each of these take in time!\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def timer(command):\n",
    "    start = time.time()\n",
    "    exec(command)\n",
    "    end = time.time()\n",
    "    return end - start\n",
    "\n",
    "#Random Forest Grid Search\n",
    "forest_param_grid = [\n",
    "    {'n_estimators':[100,200,300],\n",
    "     'max_features':[50, 100,150],\n",
    "     }\n",
    "]\n",
    "grid_search_forest = GridSearchCV(forest, forest_param_grid, cv=5, scoring='r2')\n",
    "forest_fit_time = timer('grid_search_forest.fit(X_train, y_train.ravel())')\n",
    "print 'Forest search time: ' + str(forest_fit_time)\n",
    "print\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That took over a minute to look over these hyperparameters! After seeing the results of the best parameters you might decide that you want to through more options. This could take a while and so this is best left to be done after you've gotten more juice out of your data. For demonstration purposes, let's look at what results we can get with one of our better performing models: Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_features': 100, 'n_estimators': 50}\n",
      "Mean test scores: [ 0.56120265  0.55395658  0.55901349  0.55497054]\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boost\n",
    "gradient_param_grid = [\n",
    "    {'n_estimators':[50,100],\n",
    "     'max_features':[100,200],\n",
    "     }\n",
    "]\n",
    "grid_search_gradient = GridSearchCV(gradient_boost, gradient_param_grid, cv=5, scoring='r2')\n",
    "grid_search_gradient.fit(X_train, y_train.ravel())\n",
    "print 'Best Parameters: '+ str(grid_search_gradient.best_params_)\n",
    "print 'Mean test scores: '+ str(grid_search_gradient.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we'll have to do some more parameter sweeping to find a combination that does better than our default model. We should go back and look at what we can do with the data before experimenting any further with more algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
